{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product mathiching based on ML algorithms\n",
    "\n",
    "The project is done by Basang Basangov.\n",
    "Telegram: [basan4ik](https://t.me/basan4ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Description\n",
    "\n",
    "Data matching is the workflow process of comparing different data values in structured or unstructured format based on similarity or an underlying entity [[1]](https://www.width.ai/post/data-matching-software#:~:text=How%20you%20can%20use%20machine,similarity%20or%20an%20underlying%20entity.). This notebook provides a workflow of how this type of task can be solved using two-staged process: first, FAISS similarity search is used to get 100-200 the most similar products from a database (that could petentially consist of billions of items), then among those 100-200 products get 5 that are even more similar using supervised ML algoritm, which in our case would be LightGBM classification algorithm. The final evaluation metric is accuracy@5.\n",
    "\n",
    "There are 4 files:\n",
    "- 'base.csv' is a dataset of anonymized set of products. Each product is presented with a unique id (0-base, 1-base, etc.) and vectors of the shape 1 x 72;\n",
    "- train.csv is a training dataset. Each row has an id (0-query, 1-query, etc.), a vector (1x72), and id from 'base';\n",
    "- validation.csv is a dataset of vectors that we need to find the most similar vectors from 'base'.\n",
    "- validation_answer.csv is a dataset with the right answers to the previous dataset.\n",
    "\n",
    "This is a project from [Yandex Practicum Masterskaya](https://practicum.yandex.ru/masterskaya/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if needed\n",
    "#!pip install faiss-gpu\n",
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv('data/base.csv', index_col=0)\n",
    "train = pd.read_csv('data/train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(base)\n",
    "\n",
    "base_scaled = scaler.transform(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scaled_df = pd.DataFrame(base_scaled, index=base.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train['Target']\n",
    "train_features = train.drop(['Target'], axis=1)\n",
    "\n",
    "train_features_scaled = scaler.transform(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FAISS Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 72                           # dimensions\n",
    "k_ann = 100                      # k appr. nearest neighbors\n",
    "nb = 2918139                      # database size\n",
    "nq = 10000                       # queries. intentionally decreased because of ram capacity\n",
    "np.random.seed(1234)             # make reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU...\n",
      "ntotal: 2918139\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res = faiss.StandardGpuResources() # use a single GPU\n",
    "    # make it a flat GPU index\n",
    "    index_flat = faiss.IndexFlatL2(d)   # build the index\n",
    "\n",
    "    # make it a flat GPU index\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "\n",
    "    # Then we train the index to find a suitable clustering\n",
    "    gpu_index_flat.train(np.ascontiguousarray(base_scaled.astype('float32')))\n",
    "\n",
    "    # Finally we add all embeddings to the index\n",
    "    gpu_index_flat.add(np.ascontiguousarray(base_scaled.astype('float32')))\n",
    "\n",
    "    print(f'ntotal: {gpu_index_flat.ntotal}')\n",
    "\n",
    "except:\n",
    "    print('No GPU found. Using CPU...')\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "\n",
    "    # add vectors to the index\n",
    "    index.add(np.ascontiguousarray(base_scaled.astype('float32'))) \n",
    "\n",
    "    print(f'ntotal: {index.ntotal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 FAISS Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # download prepared Faiss query indicies of 100 neighbors on full train\n",
    "    I = np.load('data/i_ndarray.npy')\n",
    "\n",
    "    # decrease query indicies as there is not enough RAM on my laptop\n",
    "    if len(I) > 10000:\n",
    "        I = I[:10000]\n",
    "except FileNotFoundError:\n",
    "    print('No prepared files are found.')\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        # searching train vectors\n",
    "        D, I = gpu_index_flat.search(np.ascontiguousarray(train_features_scaled[:nq, :].astype('float32')), k_ann)\n",
    "    except:\n",
    "        D, I = index.search(np.ascontiguousarray(train_features_scaled[:nq, :].astype('float32')), k_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.86\n"
     ]
    }
   ],
   "source": [
    "base_index = {k: v for k, v in enumerate(base.index.to_list())}\n",
    "\n",
    "acc = 0\n",
    "for target_base_name, k_closest_vectors in zip(train_target[:1000].values.tolist(), I.tolist()):\n",
    "    acc += int(target_base_name in [base_index[v] for v in k_closest_vectors])\n",
    "\n",
    "print(100 * acc / len(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGBM Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preparing Features for a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hstaked_matrix(matrix1, matrix2, index, base_df, y=None, k_ann=100, dims=72, qn=10000):\n",
    "    '''\n",
    "    Takes two scaled 3D (qn x k_ann x dims) matricies \n",
    "    and transforms them into horizontally stacked 2D matrix;\n",
    "    along with y vector if it's present\n",
    "\n",
    "    matrix1 is train or validation matrix;\n",
    "    matrix2 is a base index-filtered matrix\n",
    "    '''\n",
    "    # prepare matrix1 - tile rows\n",
    "    matrix1_tiled = np.tile(matrix1[:qn], k_ann).reshape(qn, k_ann, dims)\n",
    "    \n",
    "    # prepare matrix2\n",
    "    index_flat = index.ravel()  # flatten index\n",
    "    # take only those from base that are in flatten\n",
    "    index_base = matrix2[index_flat].reshape(qn, k_ann, dims)  # create new 3d array\n",
    "    \n",
    "    if y is not None:\n",
    "        last_dim = 1 # it needs for reshape in final result\n",
    "        if np.all(y == 1):\n",
    "            last_axis = y\n",
    "        else:\n",
    "            y_tiled = np.tile(y[:qn], k_ann)\n",
    "            I_base_names = base_df.index[index_flat]\n",
    "            last_axis = np.equal(y_tiled, I_base_names).astype('int').reshape(qn, k_ann, 1)\n",
    "\n",
    "        # Concatenate the two arrays on axis=2 along with the check_equal array \n",
    "        result = np.concatenate([matrix1_tiled, index_base, last_axis], axis=2)\n",
    "    else:\n",
    "        last_dim = 0\n",
    "        result = np.concatenate([matrix1_tiled, index_base], axis=2)\n",
    "\n",
    "    result = result.reshape(qn*k_ann, (dims*2)+last_dim)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = build_hstaked_matrix(train_features_scaled, base_scaled, I, base, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00019999999999999998"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(train_final[..., -1])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fighting class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance target classes let's augment our train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 145)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tail_90000 = train_features_scaled[10_000:, :]\n",
    "tail_base_90000 = base_scaled_df.loc[train.values[10000:, -1]]\n",
    "\n",
    "train_final_tail_90000 = np.concatenate([train_tail_90000, tail_base_90000.values, np.ones((90000, 1))], axis=1)\n",
    "\n",
    "train_final_tail_90000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = np.concatenate([train_final, train_final_tail_90000], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_final[:, :-1]\n",
    "y = train_final[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08257064220183487"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(random_state=42)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline LightGBM model\n",
    "lgbm_clf = lgb.LGBMClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "lgbm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Classifier accuracy score: 0.9445412844036697\n"
     ]
    }
   ],
   "source": [
    "print('LightGBM Classifier accuracy score:', lgbm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_csv('data/validation.csv', index_col=0)\n",
    "validation_answer = pd.read_csv('data/validation_answer.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scaled = scaler.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_ann=100\n",
    "try:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    # searching validation vectors\n",
    "    D, I = gpu_index_flat.search(np.ascontiguousarray(validation_scaled[:nq, :].astype('float32')), k_ann)\n",
    "except:\n",
    "    D, I = index.search(np.ascontiguousarray(validation_scaled[:nq, :].astype('float32')), k_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default it takes only first 10_000 rows from validation set\n",
    "validation_final = build_hstaked_matrix(validation_scaled, base_scaled, I, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "qn = 10_000\n",
    "k_ann = 100\n",
    "y_tiled = np.tile(validation_answer.iloc[:qn].values, k_ann).reshape(qn*k_ann,)\n",
    "I_base_names = base.index[I.ravel()]\n",
    "last_axis = np.equal(y_tiled, I_base_names).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Classifier accuracy score on validation set: 0.993443\n"
     ]
    }
   ],
   "source": [
    "print('LightGBM Classifier accuracy score on validation set:', lgbm_clf.score(validation_final, last_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answer_10000 = validation_answer['Expected'].iloc[:qn].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = lgbm_clf.predict_proba(validation_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prob_index = list(zip(proba[:, -1], \n",
    "                            base.index[I.ravel()].values))  # 1mln of (0.023446027463881292, '3209652-base')-like tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.69\n"
     ]
    }
   ],
   "source": [
    "# accuracy@5 metric\n",
    "acc = 0\n",
    "chunk_size = 100\n",
    "for i in range(10000):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = start_index + chunk_size\n",
    "\n",
    "    chunk = valid_prob_index[start_index:end_index]\n",
    "    top_5 = sorted(chunk, reverse=True)[:5]\n",
    "    if valid_answer_10000[i] in [t[1] for t in top_5]:\n",
    "        acc+=1\n",
    "\n",
    "print(100 * acc / len(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy@5 score on validation[:10000] is 62.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to restrictions of RAM capacity on my laptop, the decision was made to use only first 10_000 rows of train and validation datasets. Nevertheless, two-staged similarity search (FAISS + LightGBM classifier) showed that the approach is working. \n",
    "\n",
    "There are some quirks that could be changed:\n",
    "- I used FAISS IndexFlatL2 (Exact Search for L2), as it doesn't take a lot of time, but theoretically should provide the best results in terms of search quality.\n",
    "- We could use another number of approximate nearest neighbors, I used 100.\n",
    "- Using cloud solutions with larger RAM, we could perform training on a full dataset.\n",
    "- I used LightGBM classifier out of the box, other ML algorithms can be tested as well as trying various hyperparameters.\n",
    "- last but not least, the code is ugly and probably inefficient, change as you wish."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
